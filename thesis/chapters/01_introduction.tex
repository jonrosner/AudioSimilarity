% !TeX root = ../main.tex

\chapter{Introduction}\label{chapter:introduction}

In this thesis, we explore the use of deep learning and neural networks for finding similarities in audio data. For this, we use recent discoveries in the area of deep learning and combine them with established methods of signal processing to create a novel learning framework suited for the task of finding similarity patterns in the highly complex domain of audio data.

Contrastive learning, the task of learning to distinguish similar pairs of data from dissimilar ones, has become very successful in recent years to train machine learning classifiers. This was made possible mainly due to new methods leveraging on the ability to learn without the need for human-annotated examples. Instead, those methods made use of what is now known as self-supervised learning, a learning paradigm that does not require any human supervision. Most prominent in the domain of image classification was the work of Chen et al. \cite{chen2020simple} released earlier this year called "\gls{simclr}". The authors showed that their proposed framework could outperform classical methods by a large margin without the need for labeled data. Since then several papers have been published that back the strong results of \gls{simclr} and self-supervised learning in the domain of image data (\cite{grill2020bootstrap, richemond2020byol, chen2020big}). In this work, we show that similar results can be obtained in the domain of audio data with slight changes to the framework itself.

Deep neural networks have a long history in their application of compressing information from high-dimensional data to dense representations. The first major work to apply this property to problems in the domain of audio data was Hinton et al. in 2012 \cite{hinton2012speech}. After their work the use of such modern learning architectures found increasing popularity by researchers of several fields, such as speech recognition \cite{hinton2012speech}, audio classification \cite{khamparia2019soundclassification} or speaker verification \cite{chen2020vggsound}. In this work, we also make use of neural networks to create dense representations of audio data (also known as embeddings). Such embeddings can then be more easily compared for similarities by a machine. We also show that our framework is detached from the precise architecture of the neural network that is used to create the embeddings. Therefore it is possible to plug in any model architecture into the framework. We demonstrate this property by evaluating the framework using three very different modern architectures.

To make the framework applicable to several downstream tasks, such as audio classification or speech recognition, we introduce the paradigm of transfer learning into the framework. Transfer learning is a concept that was first introduced by L. Y. Pratt in 1993 \cite{NIPS1992_641}. At its core transfer learning gives a neural network the ability to take the knowledge obtained from one task and transfer it to another one. Hence we split our learning framework into two stages: $i$) A pre-training stage that uses self-supervision and a large, unlabeled dataset to learn similarities in audio data and $ii$) a transfer stage where the network trained in $i$ is transferred to three different domains to solve the downstream task of audio classification. We show that this way we beat the most recent results for few-shot classification. Few-shot classification means that the domain that is being transferred to has little or no training data, which occurs when the creation of such datasets is too difficult. The reasons for this can be manifold. For music, intellectual property rights make it impossible to create an open dataset. For the annotation of complex audio events, trained humans have to manually annotate the data in a slow, cumbersome process, making the creation of a large-scale dataset time consuming and expensive. By pre-training our network on unlabeled data we overcome this problem and make it possible to obtain good results in the transfer domain, even when labeled training data is scarce.

This thesis is structured as follows: First (Chapter \ref{chapter:preliminaries}) we briefly explain the most important preliminary knowledge in the fields of deep learning and signal processing that is required to understand this work. Afterward, we look at the most relevant related work in Chapter \ref{chapter:relatedWork}, especially at the contributions of \gls{simclr} and other self-supervised, contrastive methods. In Chapter \ref{chapter:claudio} we go into the details of our proposed framework. We explain how it is structured and do a step by step analysis of each of the necessary components. Chapter \ref{chapter:experiments} contains all the experimental setup and evaluation done to demonstrate the results of the framework. Lastly (Chapter \ref{chapter:conclusion}) we conclude the findings of this thesis and propose potential future work.

