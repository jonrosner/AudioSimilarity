% !TeX root = ../main.tex

\chapter{Related Work}\label{chapter:relatedWork}

In this chapter, we look at the previous work most relevant to ours. We first look at the concept of similarity learning for dense representations of high dimensional data. Then we focus on the progress made in the field of few-shot learning with a focus on audio classification and lastly we investigate the combination of self-supervision and transfer learning to solve the problem of few-shot classification and how it is already being successfully used in the domain of image data. We focus solely on related work on deep neural network architectures trained with stochastic gradient descent.

\section{Similarity Learning of Representations}

Similarity learning, also known as metric learning or distance learning plays an important role in many machine learning or pattern recognition tasks. A wide variety of algorithms, like \textit{K-nearest neighbor} or \textit{K-Means} can be classified under this learning paradigm since they utilize distance metrics to find patterns in datasets. In this chapter though we focus only on one particular kind of similarity learning, namely distance evaluation on dense representations of the input data created by neural networks. Dense representations, also known as embeddings, are simply low dimensional vectors that can be compared for similarity quite easily by making use of one of the distance or similarity functions defined in Section \ref{sec:ss_loss}. As explained in \ref{subsec:contrastive_learning} contrastive learning is one way of training a neural network to learn the similarity of embeddings. The modern form of the contrastive loss function used in contrastive learning was first proposed in Hadsell et al. 2006 \cite{hadsell06dimensionality}. There have been many extensions to these initial proposals, such as using a memory bank to store the representations \cite{wu2018unsupervised}. Even though these papers have shown good results they have also been criticized for becoming too complex for the marginal gains they provide \cite{chen2020simple}. Hinton et al. (2020) \cite{chen2020simple} \gls{simclr}. Their claim is that without the need for complex, specialized architectures like memory banks, good embeddings can be learned by combining self-supervised contrastive learning and data augmentation. Figure \ref{fig:simclr} shows the learning framework of \gls{simclr}. The input at each pass through the framework is a batch of unlabeled training images $x$. Each image is being transformed two times by an augmentation $t$ and another augmentation $t'$, both sampled from a distribution of augmentations $T$. Then all augmented images are passed through an encoder network $f(\cdot)$ (a \gls{resnet} \cite{he2015deep}) and a projection head $g(\cdot)$, which is simply a two-layer fully connected network. A loss function then maximizes agreement between the outputs $z_i$ and $z_j$ of the two augmented views. The authors found that the representations obtained from the encoder network $f(\cdot)$ rather than the projection head are better suited for downstream tasks and therefore after training $g(\cdot)$ is discarded.

\begin{figure}
    \centering
    \input{graphics/simclr}
     \caption[SimCLR overview]{The contrastive learning framework of \textit{SimCLR}: The input $x$ is augmentated two times by randomly sampled augmentations ($t \sim T$ and $t' \sim T$). A base encoder network $f(\cdot)$ and a projection head $g(\cdot)$ are trained using a contrastive loss to maximize agreement. Figure is from the original paper of SimCLR \cite{chen2020simple}.}
    \label{fig:simclr}
\end{figure}

\gls{simclr} uses the \gls{ntxent} loss function explained in Section \ref{subsec:ntxent}. Each image in a batch of size $N$ is augmented two times, resulting in $2N$ examples. Instead of sampling negative pairs explicitly, given a positive pair, all other $2(N-1)$ examples are treated as negative pairs. Using this loss they claim to achieve 85.8\% top-5 accuracy on only 1\% of the labels, outperforming \textit{AlexNet}, a supervised network proposed by Krizhevsky et al. \cite{NIPS2012_4824}, with 100 times fewer labels. Note that \textit{AlexNet} was the state-of-the-art model for image classification just 8 years before this paper was released.

Chung et al. \cite{chung2020defence} extensively evaluated similarity learning techniques in the realm of audio data and showed that they can outperform classification-based techniques in the task of speaker recognition. The authors also showed that the triplet loss often has slow convergence or falls into local minima.

In this work, we combine the findings of Hinton and Chung to create a self-supervised, contrastive learning framework for determining audio similarity. Compared to \gls{simclr} we propose a new set of augmentation techniques to create a suited self-supervised learning task in the domain of audio data. We also show that the triplet loss is inferior compared to the novel \gls{ntxent} loss used in \gls{simclr}, supporting the findings of Chung et al. that the triplet loss takes a long time to converge to an optimal solution \cite{chung2020defence}.

\section{Few-Shot Audio Classification}

Humans are very effective in learning new tasks with only little training data available. This is the case because we learn knowledge of the world and reapply this knowledge to other domains later on. Like a person being able to drive a car is later able to learn how to drive a truck much faster since specific concepts apply to both domains. Inspired by this and the fact that labeled data is scarce and difficult to obtain for certain fields a lot of research is now focused on training neural networks with as little data as possible. This concept is called few-shot learning and was firstly investigated in the domain of image data by Rezende et al. in 2016 \cite{rezende2016oneshot} and was later transferred to the domain of audio data by Arik et al. in 2018 \cite{arik2018neural}. An even more drastic approach is called one-shot or even zero-shot learning, where at test time only a single instance of the new classes is given to the network. Chou et al. (2019) \cite{chou2019learning} showed that using attentional similarity, few-shot sound recognition can be performed with great success. Anand et al. \cite{anand2019shot} tried to identify speakers using a prototypical loss function. They proposed to replace \glspl{cnn} with Capsule Networks, first introduced by Hinton et al. \cite{sabour2017dynamic}. Though it has not yet been shown that it can actually outperform more classical architectures. In this thesis, we show that Capsule Networks can be outperformed on few-shot classification tasks using classical established architectures like convolutional neural networks and combining them with our proposed learning framework.

\section{Self-Supervised Few-Shot Transfer Learning}

In this section, we take a closer look at the work of Medina et al. (2020) \cite{medina2020selfsupervised} which has many similarities to our proposed framework. The authors combine the two learning paradigms self-supervised learning and transfer learning to create a new solution for the few-shot classification problem. They state that several works (\cite{chen2020selfsupervised}, \cite{guo2020broader}) have shown the superiority of transfer learning compared to other methods on cross-domain settings. The authors claim that before their work, unsupervised non-episodical techniques for few-shot transfer learning have not been explored and so they propose \textit{ProtoTransfer} which "performs self-supervised pre-training on an unlabeled training domain and can transfer to few-shot target domain task" \cite{medina2020selfsupervised}. In contrast to our proposed system, they use a prototypical loss function as their self-supervised training target to minimize distances between noisy transformations of the same input. The difference between our method and \gls{simclr} is that they try to minimize the distance between one augmented image to the original image while we minimize the distance between two augmented images and therefore creating an even more difficult task for the network to learn. We chose this since we found that increasing the difficulty of the pre-training stage prevents overfitting and enables better generalization. Another difference is their fine-tuning stage, where they first calculate class-cluster centers based on the support set of the few-shot task. These centers $c$ are used as weights to a final linear layer, which is then fine-tuned using softmax cross-entropy on the few-shot dataset. We do not make use of these clusters and train a randomly initialized one layer fully connected network on the few-shot data before finally fine-tuning the entire network with a very small learning rate. We argue that this approach is simpler but at the same time yields just as good results. All in all their proposed framework is very similar to ours and draws its ideas from similar sources as we do. They showed that using this approach state-of-the-art results can be achieved in the domain of image data. Our task will be to show similar results for audio data.