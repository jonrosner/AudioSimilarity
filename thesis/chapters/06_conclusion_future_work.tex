% !TeX root = ../main.tex

\chapter{Conclusion and Future Work}\label{chapter:conclusion}

In this thesis, we investigated various subfields of machine learning. While all subfields have their own interesting peculiarities, they all have one common goal: to create strong models that generalize well to unseen data. Looking at our proposed framework we achieved exactly that. In much detail we described our learning framework called "\gls{claudio}" in Chapter \ref{chapter:claudio}. It is a general-purpose framework that can be applied to any network architecture to learn similarities in audio data.

We then used the representations learned by \gls{claudio} in a series of few-shot classification tasks. This problem of few-shot learning is very common in ML and becomes relevant every time data cannot be easily obtained or annotated. In various experiments in Chapter \ref{chapter:experiments} we showed that using this approach we outperform previous attempts of few-shot learning on audio data by as much as 10.22\%. Not only did we collect impressive results, we also had several very interesting findings along the way and many of our conducted experiments also failed to provide the expected outcome. We were especially surprised by the weak performance of the Transformer architecture in the context of audio data. While the Transformer still dominates almost all \gls{nlp} tasks, for audio it does not seem to be easily transferable. The first major problem that we encountered was with the non-existence of positional information. While traditionally this is solved using an added positional encoding signal, we found that this is not enough for the network to learn reasonable temporal dependencies in audio. Leaving the embedding out provided similar results. The convolutions required at the beginning of the network also provided troubles later on. While reducing the amount of data in two dimensions, time and frequency, convolutions add a third dimension for the different filters, called channels. Since we need two-dimensional input for the multi-head-attention block, we had to either average over the channels or flatten frequency and channels to one dimension. Since averaging failed to learn completely, flattening provided us with an even larger input-matrix. To mitigate this we introduced a dense layer that reshaped every time slot to a smaller dimension of the wanted size. Even though this worked it required a lot of fine-tuning to get this layer to learn a reasonable mapping. Lastly, we encountered the problem of obtaining a one-dimensional vector representation out of the two-dimensional output of the last \gls{mha}-block. Again we had two choices. The first option was to add a learnable vector to the input-matrix in the temporal dimension and use its last hidden state as a summary of the entire sequence. This is a similar approach as the \gls{bert} architecture \cite{devlin2019bert} uses with its $[CLS]$ token appended to each sentence. This approach did not learn a good representation though, so we were left with averaging over all time-steps. This worked but is an imperfect solution since averaging over the 300 provided time slots loses a lot of information contained in the outputs. Though all these problems were non-existent in the \gls{lstm} model, since the model was very basic, the results were also weak. The best performing model by far was the \gls{cnn}. This was to be expected since it is also the most used network for audio classification in the literature, meaning its architecture was already fine-tuned and polished by many other works while the transformer is comparably new. Looking at the problems we encountered with the transformer almost all of them are irrelevant for the \gls{cnn} based model. Even though no global temporal dependencies can be found, local ones seem to work just as well. The problem of obtaining a single vector representation is solved by averaging over the frequency axis. This works only because at this point the information is so compressed that we average over only 9 dimensions instead of 300 for the Transformer.

As for the performance of the models on the various datasets we were surprised by the discrepancy of a model's top-1 accuracy between different datasets. While all models scored the best on \textit{VoxCeleb}, every model's performance was weakest on music classification. Even though a substantial amount of the training data in the pre-training stage was music, it was still a very hard task for the network to classify the artists. The major problem was the fact that the network seemed to distinguish genre better than the artists as seen in Figure \ref{fig:tsne}. The gain of \gls{claudio} was highest in the VGG-Vox model and the \textit{VoxCeleb50-20} dataset though it can be clearly seen that all domains benefited from not initializing weights at random. We found that the amount that a model benefits do vary depending on the task. We argue that in no task should the weights be initialized at random but rather using a pre-trained encoder model and only retraining a small classification head network on the specific domain. The gains achieved towards either fully supervised methods or towards more complex architectures like the capsule network become clear when looking at the results stated in this work.

Another problem we found was that the time-stretching and pitch-shifting operations performed during augmentation required a lot of computational time, especially as batch sizes increased. This meant that the \gls{cpu} could not be fully utilized due to the fact that augmentation was done on the \gls{cpu} and therefore training time increased drastically. Propagating those costly operations to the \gls{gpu} is not easily achievable though since it requires other operations, like the \gls{stft}, to be offloaded as well.

Looking at the good results of this work, we are even more confident that the future of machine learning lies in self-supervised learning and transfer learning. The sheer amount of data that can be processed in a self-supervised way will always outperform the human-centric way of data annotation and supervised learning. But the even more important part is about the transferal of learned knowledge to a new task. For humans this is natural, but current-day ML algorithms learn knowledge from scratch on every task over and over. One can clearly see the limitations of this approach as networks become larger and integration with the real world becomes ever more important. We argue that real-world robots trained using ML will only be able to operate well if they have an excellent internal model of the real physical world that can only be obtained by reusing information from one problem to another. This combination of self-supervised learning and few-shot-transfer-learning will only become more important in the future as limits of supervised methods will be reached.

This thesis is a foundation for future works on the task of self-supervised few-shot transfer-learning in the domain of audio data. In the future this framework could be tested with even bigger, state of the art, models like the \gls{resnet} architecture proposed by He et al. \cite{he2015deep} which was already successfully applied to audio data. We believe that bigger models like the \gls{resnet} can profit even more from the benefits of pre-training using \gls{claudio}. Also, the augmentations were not yet fully tested on their individual contribution towards the final goal. Future work might look into this and see which augmentations are most important and how to set the hyperparameters for an optimal result. Maybe an increase in augmentation difficulty as the network converges can be used in a similar fashion as learning rate decay. Another interesting task that was not tackled in this thesis is verification. Verification is the task of verifying if two speakers have the same identity. Our framework would be an optimal fit for this problem.
